## ä¸€. å®éªŒåç§°

â€‹	åŸºäºå¤šç§ç¼–ç å™¨çš„æ–‡æœ¬ç†è§£



## äºŒ. å®éªŒå†…å®¹

â€‹	åˆ©ç”¨**Bi-LSTMï¼ŒTransformerï¼ŒBertç­‰æ¨¡å‹å®Œæˆæ–‡æœ¬åˆ†ç±»**ã€‚



## ä¸‰. å®éªŒå¯å‘

- å¯¹**é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ**æ—¶ï¼Œ**å­¦ä¹ ç‡å’Œåˆå§‹å‚æ•°åº”è¯¥è®¾ç½®åœ°å°ä¸€äº›**ï¼Œé¿å…é¢„è®­ç»ƒæ¨¡å‹ä¸æ”¶æ•›

- **è°ƒç”¨é¢„è®­ç»ƒè¯å‘é‡å®ç°è¯åµŒå…¥**çš„ä¸¤ç§æ–¹å¼

  ```python
  ## æ–¹å¼ä¸€ ##
      # vocab_sizeï¼šè¯æ±‡è¡¨å¤§å°
      # ninpï¼šè¯åµŒå…¥ç»´åº¦
      # embedding_weightï¼šé¢„è®­ç»ƒè¯å‘é‡
      self.embed = nn.Embedding(vocab_size, ninp)
      self.embed.weight.data.copy_(embedding_weight)
      self.embed.weight.requires_grad = True # æ˜¯å¦è¿›è¡ŒFreeze
          
      ## æ–¹å¼äºŒ ##
      # embedding_weightï¼šé¢„è®­ç»ƒè¯å‘é‡
      self.embed = nn.Embedding.from_pretrained(embedding_weight, freeze=False)
  ```

- **ä½¿ç”¨Hugging Faceç®¡é“åº”ç”¨NLPé¢„è®­ç»ƒæ¨¡å‹** (ä»¥Bertä¸ºä¾‹)

  - æ­å»ºBertæ¨¡å—

    ```python
    # BERT_PATHï¼šä»Hugging Faceä¸‹è½½ä¸‹æ¥çš„é¢„è®­ç»ƒæ¨¡å‹å‚æ•°å­˜å‚¨è·¯å¾„ï¼Œä¹Ÿå¯ä»¥ç”¨é¢„è®­ç»ƒæ¨¡å‹åç§°æ›¿æ¢ï¼Œè¿›è¡Œåœ¨çº¿ä¸‹è½½
    self.bert_config = BertConfig.from_pretrained('bert-base-uncased')
    self.bert = BertModel.from_pretrained(BERT_PATH, config = self.bert_config)
    ```

  - Bertæ¨¡å—è¾“å…¥è¾“å‡º

    ```python
    # input_idsï¼štoken id
    # attention_maskï¼šåŒºåˆ†å“ªäº›tokenæ˜¯paddingçš„ é¿å…æ¨¡å‹attentionåˆ°padding token
    # token_type_idsï¼šåŒºåˆ†ä¸åŒtoken sequence
    # bert_output[0] åŒ…å« sequence ä¸­æ‰€æœ‰ token çš„ embedding å‘é‡
    # bert_output[1] åŒ…å« [CLS] token çš„ embedding å‘é‡
    # self.bert çš„è¾“å‡ºç±»å‹æ˜¯ BaseModelOutputWithPoolingAndCrossAttentions
    bert_output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)
    ```

  - æ„é€ Bertæ¨¡å—è¾“å…¥

    ```python
    # æ„é€ åˆ†è¯å™¨
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    # textï¼šå¾…å¤„ç†æ–‡æœ¬
    # add_special_tokensï¼šæ˜¯å¦æ·»åŠ [CLS]ç­‰special token
    # paddingï¼šæ˜¯å¦paddingå¾…å¤„ç†æ–‡æœ¬
    # truncationï¼šå½“å¾…å¤„ç†æ–‡æœ¬é•¿åº¦è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œæ˜¯å¦è¿›è¡Œæˆªæ–­å¤„ç†
    # max_lengthï¼šå¾…å¤„ç†æ–‡æœ¬é•¿åº¦é˜ˆå€¼
    token = tokenizer(text, add_special_tokens = True, padding = 'max_length', truncation = True, max_length = 150)
    input_ids = token['input_ids']
    token_type_ids = token['token_type_ids']
    attention_mask = token['attention_mask']
    ```

- ç”µè„‘èµ„æºä¸å¤Ÿçš„è¯ï¼ŒAutoDLã€è…¾è®¯äº‘ç­‰æ˜¯æ—¶å€™ç™»åœºäº†ğŸŒ



## å››. å®éªŒå‚è€ƒèµ„æ–™

- LSTM åå‘ä¼ æ’­å…¬å¼æ¨å¯¼

  [LSTMå‚æ•°æ›´æ–°æ¨å¯¼ | è®°å½•æ€è€ƒ (ilewseu.github.io)](https://ilewseu.github.io/2018/01/06/LSTMå‚æ•°æ›´æ–°æ¨å¯¼/)

- Transformer Positional Encoding åŸç†ä¸æ¨å¯¼

  [ä¸€æ–‡æ•™ä½ å½»åº•ç†è§£Transformerä¸­Positional Encoding - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/338592312)

  [æ·±åº¦å­¦ä¹ å…¥é—¨--Transformerä¸­çš„Positional Encodingè¯¦è§£_class positionalencoding(nn.module):_CuddleSabeçš„åšå®¢-CSDNåšå®¢](https://blog.csdn.net/qq_15534667/article/details/116140592)

- åŠ è½½æ¨¡å‹é¢„è®­ç»ƒå‚æ•°

  [PyTorchï¼šå¦‚ä½•åŠ è½½é¢„è®­ç»ƒå‚æ•°ï¼Ÿ_torch.loadè½½å…¥è®­ç»ƒå‚æ•°-CSDNåšå®¢](https://blog.csdn.net/fhcfhc1112/article/details/95862915)

  
